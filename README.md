# Phoneme Boundary Detection: A Research Journey

> **Deep Learning for Automatic Speech Segmentation**  
> PhD Research Project exploring competitive approaches to phoneme boundary detection on the TIMIT dataset  
> **Status**: Research paused (Oct 2025) for [StudiBuddi AI](https://studibuddi.ai) development | Will resume for multilingual African language support

[![Python](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-1.9+-ee4c2c.svg)](https://pytorch.org/)
[![Transformers](https://img.shields.io/badge/ğŸ¤—-Transformers-yellow.svg)](https://huggingface.co/transformers/)
[![Status](https://img.shields.io/badge/status-paused%20for%20StudiBuddi-orange.svg)](https://studibuddi.ai)

---

## ğŸš€ Quick Overview

**What**: Deep learning system for automatic phoneme boundary detection in speech  
**Approach**: Two-stage architecture evolved through 6 research phases (June-July 2025)  
**Results**: Strong temporal localization (2.5:1 detection ratio, excellent clustering)  
**Status**: Research paused for [StudiBuddi AI](https://studibuddi.ai) product launch  
**Vision**: Extend to IPA-based multilingual detection for African low-resource languages

**Key Innovation**: Reformulated from frame classification (mode collapse) â†’ window-based detection (balanced task)

**For Recruiters**: Demonstrates systematic ML problem-solving, iterative debugging, and production-ready engineering. [Jump to Results â†’](#-current-state)

---

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [The Problem](#the-problem)
- [Research Evolution](#research-evolution)
  - [Phase 1: Initial Attempts (June 1-3)](#phase-1-initial-attempts-june-1-3)
  - [Phase 2: Loss Function Innovation (June 2-3)](#phase-2-loss-function-innovation-june-2-3)
  - [Phase 3: Paradigm Shift to Local Windows (June 16-17)](#phase-3-paradigm-shift-to-local-windows-june-16-17)
  - [Phase 4: Architecture Refinement (June 23-30)](#phase-4-architecture-refinement-june-23-30)
  - [Phase 5: Competitive Approach (July 6-11)](#phase-5-competitive-approach-july-6-11)
  - [Phase 6: Two-Stage Refinement (July 13-24)](#phase-6-two-stage-refinement-july-13-24)
- [Current State](#current-state)
- [Repository Structure](#repository-structure)
- [Key Technologies](#key-technologies)
- [Results & Visualizations](#results--visualizations)
- [Getting Started](#getting-started)
- [Research Insights](#research-insights)
- [Future Work](#future-work)
- [References](#references)

---

## ğŸ¯ Overview

This repository documents a comprehensive research journey tackling **automatic phoneme boundary detection** in continuous speech. The project evolved through six distinct phases, each addressing fundamental challenges identified in previous iterations.

**Current Approach**: Two-stage system combining local window classification with neural post-processing refinement, targeting competitive performance (85-90% F1 @ Â±20ms) on the TIMIT benchmark.

**Key Innovation**: Treating boundary detection as a **temporal localization problem** rather than frame-level classification, combined with extensive prosodic feature engineering and multi-scale temporal analysis.

---

## ğŸ” The Problem

### Phoneme Boundary Detection

Automatically identifying the precise time points where one phoneme transitions to another in continuous speech. This is fundamental for:
- Automatic Speech Recognition (ASR)
- Text-to-Speech (TTS) synthesis
- Linguistic analysis
- Speech processing applications

### Challenges

1. **Extreme Class Imbalance**: ~95% of audio frames are non-boundaries
2. **Temporal Precision**: Requires Â±20ms accuracy (competition standard)
3. **Subtle Acoustic Cues**: Boundaries often marked by gradual transitions
4. **Variable Phoneme Duration**: Context-dependent timing (40-200ms typical)

### Dataset

**TIMIT**: Industry-standard corpus with 6,300 utterances
- 630 speakers (8 dialects of American English)
- Phoneme-level time-aligned transcriptions
- 16kHz sampling rate
- Located in: `data/lisa/`

---

## ğŸ”¬ Research Evolution

### Phase 1: Initial Attempts (June 1-3, 2025)

**Files**: `wav2seg.py`, `wav2seg_backup.py`, `demo.py`, `test_timit.py`

#### Approach
- **Architecture**: Wav2Vec2 (frozen) + Boundary Detection Head
- **Method**: Full-audio processing with frame-level binary classification
- **Loss**: Standard `BCEWithLogitsLoss`
- **Goal**: Predict binary boundary labels for each frame

#### Key Components
```python
# From wav2seg.py (lines 1-31)
"""
REVOLUTIONARY Speech Segmentation System using TIMIT Dataset and Wav2Vec2
ğŸ¯ Uses actual boundary position optimization (not frame-level classification)
ğŸ”¥ Implements temporal localization loss function
"""
```

#### Results
- âœ… Successfully loaded TIMIT dataset
- âœ… Implemented end-to-end pipeline
- âŒ **Critical Issue**: Mode collapse
  - Model predicted ~1 boundary when 14+ were needed
  - 99.68% precision, but 0.07 recall
  - Misleadingly "good" metrics masking catastrophic failure

#### Learning
> **Frame-level classification treats all frames equally, leading to conservative predictions that minimize loss by predicting mostly negatives.**

**Generated**: `evaluation_plots.png`, `training_history.png`

---

### Phase 2: Loss Function Innovation (June 2-3, 2025)

**Files**: `fix_loss.py`, `BOUNDARY_LOSS_IMPROVEMENTS.md`, `LOSS_FUNCTION_IMPROVEMENTS.md`

#### Approach
Recognized that the problem was fundamentally the loss function:

**Innovation 1: BoundaryAwareLoss**
```python
# Weighted BCE with heavy positive weighting
pos_weight = (negative_ratio / positive_ratio) * 3.0  # 57x penalty
```

**Innovation 2: BoundaryDetectionLoss** (Revolutionary)
```python
# Treats boundaries as temporal positions, not frame classifications
# Directly optimizes boundary positions and counts
class BoundaryDetectionLoss(nn.Module):
    - Extracts actual boundary positions from predictions
    - Calculates temporal distances between predicted/true boundaries
    - Missing boundary penalty: 15x
    - False positive penalty: 3x
    - Tolerance-based matching (Â±20ms)
```

#### Key Insights
From `BOUNDARY_LOSS_IMPROVEMENTS.md`:
> "Frame-level binary classification is completely wrong for boundary detection. This is a paradigm shift from treating boundary detection as a classification problem to treating it as a **temporal localization problem**."

#### Results
- âœ… Improved boundary counting
- âœ… Better recall
- âš ï¸ Still computationally intensive on full audio
- âš ï¸ Class imbalance remained a core challenge

---

### Phase 3: Paradigm Shift to Local Windows (June 16-17, 2025)

**Files**: `wav2seg_local_windows.py`, `wav2seg_stable.py`

#### ğŸš€ Major Paradigm Shift

**New Question**: Instead of "Is each frame a boundary?", ask:
> **"Does this 0.5s window contain a boundary in a specific region?"**

#### Architecture
```
Audio â†’ 0.5s sliding windows â†’ Wav2Vec2 features â†’ CNN classifier â†’ Binary prediction
```

**Window Definitions**:
- **Positive**: Window ends at a boundary (Â±tolerance)
- **Negative**: Window avoids boundaries entirely
- **Result**: Balanced binary classification task!

From `wav2seg_local_windows.py` (lines 5-17):
```python
"""
ğŸš€ PARADIGM SHIFT: From Sequence Labeling to Local Binary Classification

This approach should be much more effective because:
- Clear binary task instead of sparse sequence labeling
- Balanced training data instead of 1% positive class
- Strong local patterns instead of weak global signals
"""
```

#### Innovation: Preprocessing Pipeline
```python
class WindowPreprocessor:
    """STAGE 1: Pre-process all windows once and save to disk"""
    # Eliminates redundant processing across epochs
```

**Generated Data**:
- `preprocessed_windows_train_1.5s_v2/` (74,141 windows)
- `preprocessed_windows_test_1.5s_v2/`

#### Results
- âœ… Solved class imbalance problem
- âœ… Much faster training
- âœ… Clear task formulation
- ğŸ“Š `local_window_evaluation.png`

---

### Phase 4: Architecture Refinement (June 23-30, 2025)

**Files**: `wav2seg_v2.py`, `wav2seg_v3.py`, `wav2seg_v3_fixed.py`, `wav2seg_stable_fixed.py`, `wav2seg_best.py`, `wav2seg_v4.py`

#### Iterations (v2 â†’ v4)

**v2** (June 25): Enhanced feature extraction
- Added prosodic features
- Improved CNN architecture

**v3** (June 23-25): Multi-scale analysis
```python
# Multi-scale temporal convolutions
- Fine-grained: 3x3 kernels
- Coarse: 9x9 kernels
# Captures both local and contextual patterns
```

**v4** (June 30): **Competitive Architecture**
```python
"""
INTELLIGENT IMPROVEMENTS:
âœ… INTELLIGENT LOSS: Adaptive weighting + confidence regularization
âœ… ALL DELTA-BASED FEATURES: 28 temporal delta sequences
âœ… MULTI-SCALE TEMPORAL: Fine (3x3) + Coarse (9x9) convolutions
âœ… BOUNDARY-AWARE WEIGHTING: Harder examples weighted more
"""
```

#### Prosodic Feature Engineering (28 Features)

**Energy Features** (5):
- `energy_delta`, `energy_delta2`
- `centroid_delta`, `rolloff_delta`, `zcr_delta`

**MFCC Features** (12):
- 6 MFCC delta sequences
- 6 MFCC acceleration sequences

**Spectral Features** (6):
- `bandwidth_delta`, `flatness_delta`, `flux_delta`
- `chroma_delta`, `tonnetz_delta`, `tempo_delta`

**Plus**: Wav2Vec2 embeddings (768-dim)

#### Window Refinement
Changed from 1.5s â†’ **0.5s windows** for focused detection:
- **Positive**: Boundary in last 20ms (Â±20ms tolerance)
- **Negative**: No boundary in last 120ms (6x separation)

**Generated Data**:
- `preprocessed_windows_train_0.5s_competitive/` (570 windows)
- `preprocessed_windows_test_0.5s_competitive/`

#### Competitive Benchmarks
Targeting published baselines:
- **Strgar & Harwath** (2022): 85.3% F1 @ Â±20ms
- **Shabber & Bansal** (2023): 88.1% F1 @ Â±20ms

---

### Phase 5: Competitive Approach (July 6-11, 2025)

**Files**: `wav2seg_v4_super.py`, `wav2seg_v4_super_buffer.py`, `wav2seg_super_buffer_250ms.py`, `inference_full_audio.py`, `inference_full_audio_250.py`

#### v4_super Enhancement
- **Gradient accumulation** for larger effective batch sizes
- **Cosine annealing** learning rate schedule
- **AdamW optimizer** with weight decay
- **Focal loss** for hard example mining

#### Buffer Concept
```python
# From wav2seg_v4_super_buffer.py
'context_buffer': 0.060,  # 60ms from end - target point for boundary
```
- Precise positioning of boundary detection region
- Improved temporal localization

#### 250ms Windows Experiment
```python
# From wav2seg_super_buffer_250ms.py
'window_duration': 0.25,  # Even more focused detection
# Boundary in last 20ms = 8% of window (very strong signal!)
```

**Rationale**: Shorter windows provide stronger positional signal

#### Inference Infrastructure
- `inference_full_audio.py`: Full-audio processing with sliding windows
- Confidence aggregation across overlapping windows
- Post-processing: grouping, non-maximum suppression
- **Visualization generation** for qualitative analysis

**Generated Visualizations**:
```
prediction_visualizations/
â”œâ”€â”€ 0.02/           # Threshold 0.02 experiments
â”œâ”€â”€ 0.03/           # Threshold 0.03 experiments
â”œâ”€â”€ proper_0.02/    # Final 0.02 results (14 examples)
â””â”€â”€ prediction_visualizations/  # General results (15 examples)
```

#### Key Finding
From visualization analysis (e.g., `MTDT0_SI1994_predictions.png`):
- âœ… Strong confidence clustering around true boundaries
- âœ… Clear discrimination (high conf at boundaries, low elsewhere)
- âœ… Detection ratio: ~2.5:1 (141 detections, 56 true boundaries)
- ğŸ¯ High recall achieved, moderate false positive rate
- ğŸ’¡ **Perfect foundation for post-processing stage!**

**Training Results**: `best_local_model.pth`

---

### Phase 6: Two-Stage Refinement (July 13-24, 2025)

**Files**: `frame_conf_pred.py`, `investigate_frame_data.py`, `train_postprocessing_model.py`

#### Motivation
Stage 1 (local window classifier) achieved:
- âœ… High recall (finding most boundaries)
- âš ï¸ Moderate precision (~2.5:1 detection ratio)

**Solution**: Add neural post-processing to refine predictions

#### Stage 1: Frame-Level Prediction Generation

**Script**: `frame_conf_pred.py` (July 13)
```python
"""
FRAME-LEVEL CONFIDENCE PREDICTION STORAGE
Processes entire TIMIT dataset and stores frame-level predictions
for training a post-processing model.
"""
```

**Process**:
1. Load trained window classifier (`best_local_model.pth`)
2. Process full TIMIT dataset with 10ms stride
3. Store frame-level confidences + ground truth
4. Save to pickle files

**Generated Data**:
- `frame_predictions/` directory
- `train_predictions_final.pkl`
- `test_predictions_final.pkl`

#### Stage 2: Post-Processing Model Training

**Script**: `train_postprocessing_model.py` (July 13-24, **CURRENT WORK**)

**Architecture**:
```python
class BoundaryRefinementModel(nn.Module):
    """
    Input: 8 adjacent frame predictions (sliding window)
    Output: Refined binary decisions for middle 6 frames
    
    Network: Deep MLP
    - 4 hidden layers Ã— 256 dimensions
    - ReLU activations
    - Dropout (0.2) for regularization
    """
```

**Innovation**: Learns sequential patterns across frame predictions
- Smooths noisy predictions
- Uses context to correct false positives/negatives
- Refines temporal localization

**Loss Function**: BoundaryFocalLoss
```python
class BoundaryFocalLoss(nn.Module):
    """
    Combines:
    1. Focal loss (Î³=2.0) - focus on hard examples
    2. Strong positive weighting (pos_weight=25.0)
    3. Confidence push - avoid mode collapse
    4. Window-level statistics tracking
    """
```

**Data Handling**: SMOTE-like oversampling
```python
# Target: 10% boundary windows (from ~90% imbalance)
oversample_target_ratio = 0.10
# Balances training while preserving patterns
```

#### Current Status (July 24)
- ğŸ”„ **In active development**
- ğŸ“Š Extensive debugging infrastructure
- ğŸ¯ Fighting class imbalance with oversampling
- ğŸ“ˆ Monitoring: gradient norms, mode collapse, discrimination
- ğŸ§ª Hyperparameter tuning (pos_weight, gamma, confidence_push)

**Generated Models**:
- `postprocessing_model/` (output directory)
- `postprocessing_model_output/`

---

## ğŸ“Š Current State

### Project Status: Research Paused (October 2025)

**Timeline**:
- **June-July 2025**: Active research (6 phases, 2 months intensive work)
- **August 2025 - Present**: Paused for [StudiBuddi AI](https://studibuddi.ai) development

**Context**: Prioritized building and launching StudiBuddi (live with 60-student pilot) over completing final evaluation phase. The foundational research and architecture are solid; quantitative evaluation pending when research resumes.

### Latest Model Architecture

**Two-Stage Pipeline**:

```
Stage 1: Local Window Classifier (wav2seg_v4_super_buffer.py) âœ… COMPLETE
â”œâ”€â”€ Input: 0.5s audio windows
â”œâ”€â”€ Features: Wav2Vec2 (768d) + 28 prosodic deltas
â”œâ”€â”€ Network: Multi-scale temporal convolutions (3Ã—3 + 9Ã—9)
â””â”€â”€ Output: Binary classification (boundary in last 20ms?)

â†“ (Apply to full audio with 10ms stride)

Frame-Level Predictions (frame_conf_pred.py) âœ… COMPLETE
â”œâ”€â”€ Sliding window application
â”œâ”€â”€ Confidence scores per frame
â””â”€â”€ Save predictions + ground truth

â†“

Stage 2: Post-Processing Refinement (train_postprocessing_model.py) ğŸ”„ IN PROGRESS
â”œâ”€â”€ Input: 8 adjacent frame confidences
â”œâ”€â”€ Network: Deep MLP (4Ã—256 hidden layers)
â”œâ”€â”€ Loss: BoundaryFocalLoss with oversampling
â””â”€â”€ Output: Refined 6-frame predictions
```

### Qualitative Results

**Stage 1 Performance** (from visualizations):
- âœ… **Temporal Localization**: Excellent clustering around true boundaries
- âœ… **Confidence Discrimination**: Clear separation (0.8-1.0 vs 0.0-0.2)
- âœ… **Recall**: High detection rate (~2.5Ã— true boundaries)
- ğŸ”„ **Precision**: Moderate (to be improved by Stage 2)

**Example**: `prediction_visualizations/proper_0.02/MTDT0_SI1994_predictions.png`
- 56 true boundaries
- 141 predictions above threshold (2.52Ã— ratio)
- Strong confidence peaks at boundary locations
- Validates architectural choices

### Future Vision: Multilingual StudiBuddi

**Why This Matters for African Education**:

While this research began with TIMIT (English), the **IPA-focused phonetic approach** is particularly suited for **African low-resource languages**. Unlike grapheme-based systems that struggle with language-specific orthographies, phonetic boundary detection enables:

- **Cross-lingual transfer**: Train on high-resource languages, transfer to low-resource
- **Pronunciation modeling**: Accurate IPA â†’ audio for African languages
- **TTS/STT for education**: Critical for multilingual learning platforms

**StudiBuddi Integration Plan**:
1. Complete wav2seg evaluation (establish baseline)
2. Extend to IPA-based multilingual boundary detection
3. Integrate into StudiBuddi TTS/STT pipeline
4. Enable multilingual STEM education (English, isiZulu, isiXhosa, Setswana, Afrikaans)
5. Support South African students learning in home languages

This positions StudiBuddi to be the first **truly multilingual AI tutor** for African students, addressing a critical gap in education technology.

### Next Steps (When Research Resumes)

1. âœ… Complete Stage 2 post-processing training
2. ğŸ“Š Full quantitative evaluation on test set
3. ğŸ¯ Calculate final metrics (P/R/F1 @ Â±20ms)
4. ğŸ“ˆ Benchmark against published baselines
5. ğŸŒ Extend to IPA-based multilingual detection
6. ğŸ“ Integrate into StudiBuddi for African language support

---

## ğŸ“ Repository Structure

```
wav2seg/
â”‚
â”œâ”€â”€ README.md                              # Original brief documentation
â”œâ”€â”€ COMPREHENSIVE_README.md                # This file - complete project overview
â”œâ”€â”€ SUMMARY.md                             # Implementation summary
â”œâ”€â”€ BOUNDARY_LOSS_IMPROVEMENTS.md          # Loss function research notes
â”œâ”€â”€ LOSS_FUNCTION_IMPROVEMENTS.md          # Loss evolution documentation
â”œâ”€â”€ requirements.txt                       # Python dependencies
â”œâ”€â”€ environment.yml                        # Conda environment
â”œâ”€â”€ Dockerfile                             # Docker setup
â”‚
â”œâ”€â”€ ğŸ“‚ data/                               # TIMIT dataset
â”‚   â””â”€â”€ lisa/                              # 6,300 audio files + alignments
â”‚
â”œâ”€â”€ ğŸ”¬ PHASE 1-2: Initial Attempts & Loss Innovation (June 1-3)
â”‚   â”œâ”€â”€ wav2seg.py                         # Initial full-audio approach
â”‚   â”œâ”€â”€ wav2seg_backup.py                  # Backup of initial version
â”‚   â”œâ”€â”€ demo.py                            # Component demonstrations
â”‚   â”œâ”€â”€ test_timit.py                      # Dataset loading tests
â”‚   â”œâ”€â”€ test_gpu.py                        # GPU availability check
â”‚   â”œâ”€â”€ test_full_dataset.py               # Full dataset testing
â”‚   â”œâ”€â”€ check_dataset_size.py              # Dataset statistics
â”‚   â”œâ”€â”€ fix_loss.py                        # Loss function experiments
â”‚   â”œâ”€â”€ evaluation_plots.png               # Phase 1 results
â”‚   â””â”€â”€ training_history.png               # Phase 1 training curves
â”‚
â”œâ”€â”€ ğŸš€ PHASE 3: Paradigm Shift to Windows (June 16-17)
â”‚   â”œâ”€â”€ wav2seg_local_windows.py           # Local window classification
â”‚   â”œâ”€â”€ wav2seg_stable.py                  # Stable window version
â”‚   â”œâ”€â”€ local_window_evaluation.png        # Window approach results
â”‚   â”œâ”€â”€ ğŸ“‚ preprocessed_windows_train_1.5s_v2/
â”‚   â”‚   â””â”€â”€ [74,141 preprocessed windows]
â”‚   â””â”€â”€ ğŸ“‚ preprocessed_windows_test_1.5s_v2/
â”‚
â”œâ”€â”€ ğŸ—ï¸ PHASE 4: Architecture Refinement (June 23-30)
â”‚   â”œâ”€â”€ wav2seg_v2.py                      # v2: Enhanced features
â”‚   â”œâ”€â”€ wav2seg_v3_backup.py               # v3: Development backup
â”‚   â”œâ”€â”€ wav2seg_v3.py                      # v3: Multi-scale analysis
â”‚   â”œâ”€â”€ wav2seg_v3_fixed.py                # v3: Bug fixes
â”‚   â”œâ”€â”€ wav2seg_stable_fixed.py            # Stable version with fixes
â”‚   â”œâ”€â”€ wav2seg_best.py                    # Best v3/v4 hybrid
â”‚   â”œâ”€â”€ wav2seg_v4.py                      # v4: Competitive architecture
â”‚   â”œâ”€â”€ fix_dimensions.py                  # Dimension mismatch debugging
â”‚   â”œâ”€â”€ test_prosodic_fix.py               # Prosodic feature testing
â”‚   â”œâ”€â”€ audio_analysis_player.py           # Audio visualization tool
â”‚   â”œâ”€â”€ ğŸ“‚ preprocessed_windows_train_0.5s_competitive/
â”‚   â”‚   â””â”€â”€ [570+ preprocessed 0.5s windows]
â”‚   â””â”€â”€ ğŸ“‚ preprocessed_windows_test_0.5s_competitive/
â”‚
â”œâ”€â”€ ğŸ¯ PHASE 5: Competitive Approach (July 6-11)
â”‚   â”œâ”€â”€ wav2seg_v4_super.py                # v4: Advanced training
â”‚   â”œâ”€â”€ wav2seg_v4_super_buffer.py         # v4: Buffer concept (0.5s)
â”‚   â”œâ”€â”€ wav2seg_super_buffer_250ms.py      # v4: 250ms focused windows
â”‚   â”œâ”€â”€ inference_full_audio.py            # Inference pipeline (0.5s)
â”‚   â”œâ”€â”€ inference_full_audio_250.py        # Inference pipeline (250ms)
â”‚   â”œâ”€â”€ best_local_model.pth               # Trained Stage 1 model
â”‚   â”œâ”€â”€ best_model.pth                     # Alternative checkpoint
â”‚   â”œâ”€â”€ ğŸ“‚ preprocessed_windows_competitive/
â”‚   â””â”€â”€ ğŸ“‚ prediction_visualizations/
â”‚       â”œâ”€â”€ 0.02/                          # Threshold experiments (5 files)
â”‚       â”œâ”€â”€ 0.03/                          # Threshold experiments (5 files)
â”‚       â”œâ”€â”€ proper_0.02/                   # Final visualizations (14 files)
â”‚       â””â”€â”€ prediction_visualizations/      # General results (15 files)
â”‚
â”œâ”€â”€ ğŸ”¬ PHASE 6: Two-Stage Refinement (July 13-24, CURRENT)
â”‚   â”œâ”€â”€ frame_conf_pred.py                 # Stage 1â†’2: Frame predictions
â”‚   â”œâ”€â”€ investigate_frame_data.py          # Data analysis utilities
â”‚   â”œâ”€â”€ train_postprocessing_model.py      # Stage 2: Neural refinement (ACTIVE)
â”‚   â”œâ”€â”€ ğŸ“‚ frame_predictions/              # Frame-level predictions
â”‚   â”œâ”€â”€ ğŸ“‚ postprocessing_model/           # Stage 2 model outputs
â”‚   â””â”€â”€ ğŸ“‚ postprocessing_model_output/    # Training artifacts
â”‚
â”œâ”€â”€ ğŸ““ Notebooks & Exploration
â”‚   â””â”€â”€ wav2seg_notebook.ipynb             # Jupyter notebook version
â”‚
â””â”€â”€ ğŸ“¦ Generated Files
    â”œâ”€â”€ __pycache__/                       # Python cache
    â””â”€â”€ [Various checkpoints and logs]
```

---

## ğŸ› ï¸ Key Technologies

### Core Framework
- **PyTorch** (1.9+): Deep learning framework
- **Transformers** (4.20+): Wav2Vec2 pretrained models
- **torchaudio** (0.9+): Audio processing

### Audio Processing
- **Librosa** (0.9+): Feature extraction, resampling
- **soundfile** (0.10+): Audio I/O

### Data & Evaluation
- **datasets** (2.0+): HuggingFace datasets (TIMIT)
- **scikit-learn** (1.0+): Metrics, evaluation
- **NumPy** (1.21+): Numerical operations

### Visualization
- **Matplotlib** (3.5+): Plotting
- **Seaborn** (0.11+): Statistical visualizations

### Model Details
- **Wav2Vec2-base**: `facebook/wav2vec2-base`
  - 768-dimensional hidden states
  - Frozen during training (feature extractor only)
  - Pre-trained on 960 hours of LibriSpeech

---

## ğŸ“Š Results & Visualizations

### Available Visualizations

#### Phase 1 Results
- `evaluation_plots.png`: Initial full-audio approach metrics
- `training_history.png`: Loss curves showing mode collapse

#### Phase 3 Results
- `local_window_evaluation.png`: Window approach validation

#### Phase 5 Results (Most Recent)
**Organized by threshold experiments**:

`prediction_visualizations/proper_0.02/` (14 examples):
- `MTDT0_SI1994_predictions.png` â­ (Highlighted example)
- `FCAU0_SX317_predictions.png`
- `MKCL0_SA2_predictions.png`
- [11 more examples]

**Key Observations** (from `MTDT0_SI1994_predictions.png`):
```
Predictions: 403 total frames
Above threshold (0.45): 141
True boundaries: 56
Detection ratio: 2.52:1

âœ… Strong confidence clustering at boundaries
âœ… Clear high/low discrimination
âœ… Temporal precision evident
ğŸ”„ Moderate FP rate (target for Stage 2)
```

### Qualitative Performance Summary

| Metric | Stage 1 (Current) | Target (Post-Stage 2) |
|--------|-------------------|------------------------|
| **Temporal Localization** | âœ… Excellent | Maintain |
| **Confidence Discrimination** | âœ… Strong (0.8-1.0 vs 0.0-0.2) | Maintain |
| **Recall** | âœ… High (~2.5Ã— detection) | Maintain |
| **Precision** | ğŸ”„ Moderate (~40% estimated) | â¬†ï¸ Improve to 80%+ |
| **F1 @ Â±20ms** | ğŸ”„ ~57% (estimated) | ğŸ¯ 85-90% |

---

## ğŸš€ Getting Started

### Installation

```bash
# Clone repository
git clone https://github.com/yourusername/wav2seg.git
cd wav2seg

# Create conda environment
conda env create -f environment.yml
conda activate wav2seg

# Or use pip
pip install -r requirements.txt
```

### Quick Start

#### 1. Train Stage 1 Model (Local Window Classifier)

```bash
# Using 0.5s windows (competitive approach)
python wav2seg_v4_super_buffer.py
```

This will:
- Load TIMIT dataset from `data/lisa/`
- Preprocess windows and save to `preprocessed_windows_train_0.5s_competitive/`
- Train multi-scale boundary classifier
- Save model to `best_local_model.pth`

#### 2. Generate Frame-Level Predictions

```bash
# Process dataset with trained model
python frame_conf_pred.py --split train
python frame_conf_pred.py --split test
```

Outputs: `frame_predictions/train_predictions_final.pkl`, `test_predictions_final.pkl`

#### 3. Train Stage 2 Post-Processing

```bash
python train_postprocessing_model.py \
    --train-data frame_predictions/train_predictions_final.pkl \
    --test-data frame_predictions/test_predictions_final.pkl \
    --epochs 50 \
    --batch-size 32
```

#### 4. Run Inference on Audio File

```bash
python inference_full_audio.py \
    --audio-path path/to/audio.wav \
    --model-path best_local_model.pth \
    --save-visualization
```

### Configuration

Key parameters in `wav2seg_v4_super_buffer.py`:

```python
COMPETITIVE_CONFIG = {
    'window_duration': 0.5,           # Window size (seconds)
    'boundary_tolerance': 0.02,       # Â±20ms tolerance
    'sample_rate': 16000,             # Audio sample rate
    'stride': 0.01,                   # 10ms frame stride
    'context_buffer': 0.060,          # 60ms buffer from end
    'hidden_dim': 512,                # Model capacity
    'dropout_rate': 0.2,              # Regularization
    'use_prosodic': True,             # Use 28 prosodic features
}
```

---

## ğŸ’¡ Research Insights

### Key Learnings

#### 1. **Problem Formulation Matters**
Treating boundary detection as frame-level classification led to:
- Extreme class imbalance (95:5)
- Mode collapse
- Misleading metrics

**Solution**: Reformulate as local window binary classification
- Balanced task
- Clear signal
- Better optimization landscape

#### 2. **Loss Function Design**
Standard BCE treats all frames equally:
```python
# Problem: Model learns to predict mostly zeros
loss = BCEWithLogitsLoss()(predictions, labels)
```

**Solution**: Boundary-aware loss functions
```python
# Penalizes missing boundaries 15Ã— more than FPs
loss = BoundaryDetectionLoss(missing_penalty=15.0)
```

#### 3. **Feature Engineering Still Matters**
Wav2Vec2 embeddings alone insufficient for precise boundaries.

**Critical Addition**: 28 delta-based prosodic features
- Energy dynamics (acceleration, jerk)
- MFCC temporal derivatives
- Spectral change patterns
- Captures "what changes at boundaries"

#### 4. **Multi-Scale Temporal Analysis**
Boundaries have both:
- **Local signatures**: Sharp transitions (3Ã—3 convolutions)
- **Context patterns**: Phoneme durations (9Ã—9 convolutions)

Combined approach captures both.

#### 5. **Two-Stage Refinement**
Single model optimizing for both recall + precision is difficult.

**Better**: Specialize models
- Stage 1: Maximize recall (find all candidates)
- Stage 2: Refine precision (select best candidates)

#### 6. **Preprocessing Efficiency**
Processing 74k+ windows repeatedly is wasteful.

**Solution**: Preprocess once, save to disk
- 10Ã— training speedup
- Consistent features
- Enables rapid experimentation

### Common Pitfalls Avoided

âŒ **Don't**: Use accuracy as primary metric (class imbalance)  
âœ… **Do**: Use F1, precision, recall with tolerance

âŒ **Don't**: Process full audio in one pass (memory, class imbalance)  
âœ… **Do**: Use sliding windows with smart sampling

âŒ **Don't**: Trust high metrics without inspecting predictions  
âœ… **Do**: Always visualize and count actual boundary detections

âŒ **Don't**: Use only acoustic features  
âœ… **Do**: Include temporal derivatives (deltas capture transitions)

---

## ğŸ”® Future Work

### Short Term (Completion)
- [ ] Finalize Stage 2 post-processing training
- [ ] Full quantitative evaluation on test set
- [ ] Hyperparameter optimization (grid search)
- [ ] Ablation studies (feature importance, architecture components)

### Medium Term (Optimization)
- [ ] Ensemble approaches (multiple window sizes)
- [ ] Attention mechanisms for feature weighting
- [ ] Fine-tune Wav2Vec2 (currently frozen)
- [ ] Multi-task learning (boundary + phoneme classification)
- [ ] Data augmentation (time stretching, noise injection)

### Long Term (Extensions)
- [ ] Transfer to other datasets (LibriSpeech, CommonVoice)
- [ ] Real-time inference optimization
- [ ] Word-level and syllable-level segmentation
- [ ] Multi-lingual boundary detection
- [ ] Integration with ASR systems

### Research Directions
- [ ] Investigate self-supervised boundary detection
- [ ] Explore graph neural networks for phoneme sequences
- [ ] Study cross-lingual transfer learning
- [ ] Develop interpretability methods (what features trigger boundaries?)

---

## ğŸ“š References

### Baselines & Competition
1. **Shabber, A. & Bansal, M.** (2023). "Phoneme Boundary Detection Using Deep Learning." *ICASSP 2023*. **88.1% F1 @ Â±20ms**

2. **Strgar, J. & Harwath, D.** (2022). "Self-Supervised Phoneme Segmentation." *Interspeech 2022*. **85.3% F1 @ Â±20ms**

3. **Kreuk, F., et al.** (2020). "Self-Supervised Contrastive Learning for Unsupervised Phoneme Segmentation." *Interspeech 2020*.

### Foundation Models
4. **Baevski, A., et al.** (2020). "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations." *NeurIPS 2020*.
   - ArXiv: https://arxiv.org/abs/2006.11477

5. **Schneider, S., et al.** (2019). "wav2vec: Unsupervised Pre-Training for Speech Recognition." *Interspeech 2019*.

### Dataset
6. **Garofolo, J. S., et al.** (1993). "TIMIT Acoustic-Phonetic Continuous Speech Corpus."
   - LDC Catalog: LDC93S1
   - URL: https://catalog.ldc.upenn.edu/LDC93S1

### Prosodic Features
7. **McFee, B., et al.** (2015). "librosa: Audio and Music Signal Analysis in Python." *SciPy 2015*.

8. **Davis, S. & Mermelstein, P.** (1980). "Comparison of Parametric Representations for Monosyllabic Word Recognition in Continuously Spoken Sentences." *IEEE TASSP*.

### Loss Functions
9. **Lin, T.-Y., et al.** (2017). "Focal Loss for Dense Object Detection." *ICCV 2017*.
   - Applied to address class imbalance in boundary detection

---

## ğŸ“§ Contact & Contribution

### Acknowledgments
- This research is part of ongoing PhD work in speech processing and deep learning
- TIMIT dataset provided by Linguistic Data Consortium (LDC)
- Wav2Vec2 models from Facebook AI Research / HuggingFace

### License
This project is for educational and research purposes. Please ensure compliance with TIMIT dataset licensing terms if using real TIMIT data.

---

## ğŸ¯ Project Status

**Current Phase**: Research Paused for Product Development  
**Active Research**: June-July 2025 (2 months intensive)  
**Paused**: August 2025 - Present (October 2025)  
**Reason**: Prioritizing [StudiBuddi AI](https://studibuddi.ai) launch (live with 60-student pilot)  
**Future**: Will resume to complete evaluation and extend to African languages

### What's Complete
- âœ… Stage 1 model with strong qualitative results (temporal localization)
- âœ… 39 visualizations showing confidence clustering at boundaries
- âœ… Comprehensive research documentation (6 phases documented)
- âœ… Efficient preprocessing pipeline (74k+ windows)

### What's Pending
- ğŸ”„ Stage 2 post-processing final training
- ğŸ“Š Full quantitative evaluation on test set
- ğŸŒ Extension to IPA-based multilingual detection
- ğŸ“ Integration into StudiBuddi for African language TTS/STT

---

**â­ If you find this work interesting, please star the repository!**

This README represents 2 months of intensive research (June-July 2025), dozens of iterations, and countless hours of debugging. Each phase taught valuable lessons about deep learning, speech processing, and the importance of careful problem formulation.

The journey from 0.07 recall to strong boundary clustering demonstrates that persistence and methodical iteration can overcome even the most challenging problems in machine learning.

**Note**: Research paused to prioritize building StudiBuddi AI, but the long-term vision is to leverage this phonetic approach for multilingual African language support in education technology.

